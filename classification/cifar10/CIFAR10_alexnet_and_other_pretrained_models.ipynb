{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AlexNet CNN Architecture Using TensorFlow 2.0+ and Keras\n",
    "\n",
    "Learn how to implement the neural network architecture that kicked off the deep convolutional neural network revolution back in 2012.\n",
    "\n",
    "https://towardsdatascience.com/implementing-alexnet-cnn-architecture-using-tensorflow-2-0-and-keras-2113e090ad98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "The CIFAR-10 dataset contains 60,000 colour images, each with dimensions 32x32px. The content of the images within the dataset is sampled from 10 classes.\n",
    "\n",
    "CIFAR-10 images were aggregated by some of the creators of the AlexNet network, Alex Krizhevsky and Geoffrey Hinton.\n",
    "The deep learning Keras library provides direct access to the CIFAR10 dataset with relative ease, through its dataset module. Accessing common datasets such as CIFAR10 or MNIST, becomes a trivial task with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES= ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR dataset is partitioned into 50,000 training data and 10,000 test data by default. The last partition of the dataset we require is the validation data.\n",
    "The validation data is obtained by taking the last 5000 images within the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26250, 32, 32, 3), (26250, 1), (8750, 32, 32, 3), (8750, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation_images, validation_labels = train_images[:1000], train_labels[:1000]\n",
    "# train_images, train_labels = train_images[1000:20000], train_labels[1000:20000]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images, validation_images, train_labels, validation_labels = train_test_split(train_images, train_labels,\n",
    "                                                                                    test_size= 0.3,\n",
    "                                                                                   shuffle=True,\n",
    "                                                                                   random_state=42,\n",
    "                                                                                   stratify=train_labels)\n",
    "\n",
    "train_images, validation_images, train_labels, validation_labels = train_test_split(train_images, train_labels,\n",
    "                                                                                    test_size= 0.25,\n",
    "                                                                                   shuffle=True,\n",
    "                                                                                   random_state=42,\n",
    "                                                                                   stratify=train_labels)\n",
    "\n",
    "train_images.shape, train_labels.shape, validation_images.shape, validation_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to access these methods and procedures, it is required that we transform our dataset into an efficient data representation TensorFlow is familiar with. This is achieved using the tf.data.Dataset API.\n",
    "More specifically, tf.data.Dataset.from_tensor_slices method takes the train, test, and validation dataset partitions and returns a corresponding TensorFlow Dataset representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "Preprocessing within any machine learning is associated with the transformation of data from one form to another.\n",
    "Usually, preprocessing is conducted to ensure the data utilized is within an appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excuse the blurriness of the images; the CIFAR-10 images have small dimensions, which makes visualization of the actual pictures a bit difficult.\n",
    "plt.figure(figsize=(20,20))\n",
    "for i, (image, label) in enumerate(train_ds.take(5)):\n",
    "    ax = plt.subplot(5,5,i+1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(CLASS_NAMES[label.numpy()[0]])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary preprocessing transformations that will be imposed on the data presented to the network are:\n",
    "- Normalizing and standardizing the images.\n",
    "- Resizing of the images from 32x32 to 227x227. The AlexNet network input expects a 227x227 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image, label):\n",
    "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    # Resize images from 32x32 to 227x227\n",
    "    image = tf.image.resize(image, (227,227))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data/Input Pipeline\n",
    "An input/data pipeline is described as a series of functions or methods that are called consecutively one after another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_size = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "test_ds_size = tf.data.experimental.cardinality(test_ds).numpy()\n",
    "validation_ds_size = tf.data.experimental.cardinality(validation_ds).numpy()\n",
    "print(\"Training data size:\", train_ds_size)\n",
    "print(\"Test data size:\", test_ds_size)\n",
    "print(\"Validation data size:\", validation_ds_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our basic input/data pipeline, we will conduct three primary operations:\n",
    "- Preprocessing the data within the dataset\n",
    "- Shuffle the dataset\n",
    "- Batch data within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (train_ds\n",
    "                  .map(process_images)\n",
    "                  .shuffle(buffer_size=train_ds_size)\n",
    "                  .batch(batch_size=4, drop_remainder=True))\n",
    "test_ds = (test_ds\n",
    "                  .map(process_images)\n",
    "                  .shuffle(buffer_size=train_ds_size)\n",
    "                  .batch(batch_size=4, drop_remainder=True))\n",
    "validation_ds = (validation_ds\n",
    "                  .map(process_images)\n",
    "                  .shuffle(buffer_size=train_ds_size)\n",
    "                  .batch(batch_size=4, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Implementation\n",
    "Here are the types of layers the AlexNet CNN architecture is composed of, along with a brief description:\n",
    "\n",
    "**Convolutional layer**: A convolution is a mathematical term that describes a dot product multiplication between two sets of elements. Within deep learning the convolution operation acts on the filters/kernels and image data array within the convolutional layer. Therefore a convolutional layer is simply a layer the houses the convolution operation that occurs between the filters and the images passed through a convolutional neural network.\n",
    "\n",
    "**Batch Normalisation layer**: Batch Normalization is a technique that mitigates the effect of unstable gradients within a neural network through the introduction of an additional layer that performs operations on the inputs from the previous layer. The operations standardize and normalize the input values, after that the input values are transformed through scaling and shifting operations.\n",
    "\n",
    "**MaxPooling layer**: Max pooling is a variant of sub-sampling where the maximum pixel value of pixels that fall within the receptive field of a unit within a sub-sampling layer is taken as the output. The max-pooling operation below has a window of 2x2 and slides across the input data, outputting an average of the pixels within the receptive field of the kernel.\n",
    "\n",
    "**Flatten layer**: Takes an input shape and flattens the input image data into a one-dimensional array.\n",
    "Dense Layer: A dense layer has an embedded number of arbitrary units/neurons within. Each neuron is a perceptron.\n",
    "\n",
    "### Some other operations and techniques utilized within the AlexNet CNN that are worth mentioning are:\n",
    "\n",
    "**Activation Function**: A mathematical operation that transforms the result or signals of neurons into a normalized output. The purpose of an activation function as a component of a neural network is to introduce non-linearity within the network. The inclusion of an activation function enables the neural network to have greater representational power and solve complex functions.\n",
    "\n",
    "**Rectified Linear Unit** Activation Function(ReLU): A type of activation function that transforms the value results of a neuron. The transformation imposed by ReLU on values from a neuron is represented by the formula y=max(0,x). The ReLU activation function clamps down any negative values from the neuron to 0, and positive values remain unchanged. The result of this mathematical transformation is utilized as the output of the current layer and used as input to a consecutive layer within a neural network.\n",
    "\n",
    "**Softmax Activation Function**: A type of activation function that is utilized to derive the probability distribution of a set of numbers within an input vector. The output of a softmax activation function is a vector in which its set of values represents the probability of an occurrence of a class or event. The values within the vector all add up to 1.\n",
    "\n",
    "**Dropout**: Dropout technique works by randomly reducing the number of interconnecting neurons within a neural network. At every training step, each neuron has a chance of being left out, or rather, dropped out of the collated contributions from connected neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TensorBoard\n",
    "TensorBoard is a tool that provides a suite of visualization and monitoring mechanisms. For the work in this tutorial, we’ll be utilizing TensorBoard to monitor the progress of the training of the network.\n",
    "\n",
    "More specifically, we’ll be monitoring the following metrics: training loss, training accuracy, validation loss, validation accuracy.\n",
    "\n",
    "In the shortcode snippet below we are creating a reference to the directory we would like all TensorBoard files to be stored within. The function get_run_logdir returns the location of the exact directory that is named according to the current time the training phase starts.\n",
    "\n",
    "To complete this current process, we pass the directory to store TensorBoard related files for a particular training session to the TensorBoard callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"logs\",\"fit\",\"\")\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "To train the network, we have to compile it.\n",
    "\n",
    "The compilation processes involve specifying the following items:\n",
    "\n",
    "**Loss function**: A method that quantifies ‘how well’ a machine learning model performs. The quantification is an output(cost) based on a set of inputs, which are referred to as parameter values. The parameter values are used to estimate a prediction, and the ‘loss’ is the difference between the predictions and the actual values.\n",
    "\n",
    "**Optimization Algorithm**: An optimizer within a neural network is an algorithmic implementation that facilitates the process of gradient descent within a neural network by minimizing the loss values provided via the loss function. To reduce the loss, it is paramount the values of the weights within the network are selected appropriately.\n",
    "\n",
    "**Learning Rate**: An integral component of a neural network implementation detail as it’s a factor value that determines the level of updates that are made to the values of the weights of the network. Learning rate is a type of hyperparameter.\n",
    "\n",
    "In Fit method: \n",
    "\n",
    "**Epoch**: This is a numeric value that indicates the number of time a network has been exposed to all the data points within a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.optimizers.SGD(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds,\n",
    "          epochs=50,          \n",
    "          validation_data=validation_ds,\n",
    "          validation_freq = 1,\n",
    "          workers = 48,\n",
    "          callbacks=[tensorboard_cb],\n",
    "          use_multiprocessing=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Saved_model/CIFAR10_Alexnet_50_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "After executing the cell block below, we are presented with a score that indicates the performance of the model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element of the returned result contains the evaluation loss: 1.115, the second element indicates is the evaluation accuracy 0.785."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom implemented AlexNet network that was trained, validated, and evaluated on the CIFAR-10 dataset to create a model with an evaluation accuracy of 78.5% on a test dataset containing 2500 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # total_images = 10000\n",
    "# # batch size = 10\n",
    "# # steps per epoch = 1000\n",
    "# # epochs = 1\n",
    "\n",
    "# total_images :\n",
    "#     train = 26250\n",
    "#     val = 8750\n",
    "# batch size = 4\n",
    "# epochs = 50\n",
    "# steps per epoch = 6562\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.7.0 at http://0.0.0.0:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir logs --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pretrained weights\n",
    "https://keras.io/api/applications/#available-models\n",
    "Let’s learn how to classify images with pre-trained Convolutional Neural Networks using the Keras library.\n",
    "\n",
    "Keras Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\n",
    "\n",
    "Weights are downloaded automatically when instantiating a model. They are stored at ~/.keras/models/.\n",
    "\n",
    "Upon instantiation, the models will be built according to the image data format set in your Keras configuration file at ~/.keras/keras.json. For instance, if you have set image_data_format=channels_last, then any model loaded from this repository will get built according to the TensorFlow data format convention, \"Height-Width-Depth\".\n",
    "\n",
    "### Note: each Keras Application expects a specific kind of input preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default input size for this model is 224x224.\n",
    "# Note: each Keras Application expects a specific kind of input preprocessing. \n",
    "# For VGG16, call tf.keras.applications.vgg16.preprocess_input on your inputs before passing them to the model. \n",
    "# vgg16.preprocess_input will convert the input images from RGB to BGR, \n",
    "# then will zero-center each color channel with respect to the ImageNet dataset, without scaling.\n",
    "\n",
    "# include_top: whether to include the 3 fully-connected layers at the top of the network.\n",
    "# weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded.\n",
    "# input_tensor: optional Keras tensor (i.e. output of layers.Input()) to use as image input for the model.\n",
    "# input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 input channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value.\n",
    "# pooling: Optional pooling mode for feature extraction when include_top is False. - None means that the output of the model will be the 4D tensor output of the last convolutional block. - avg means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. - max means that global max pooling will be applied.\n",
    "# classes: optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified.\n",
    "# classifier_activation: A str or callable. The activation function to use on the \"top\" layer. Ignored unless include_top=True. Set classifier_activation=None to return the logits of the \"top\" layer. When loading pretrained weights, classifier_activation can only be None or \"softmax\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"logs\",\"fit\",\"\")\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16, VGG19\n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Flatten,Dense\n",
    "\n",
    "def get_model(model_name, input_shape_=(32,32,3), n_class=10, last_act_func=\"softmax\"):\n",
    "    pretrained_model = model_name(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=None,\n",
    "        input_shape=input_shape_,\n",
    "        pooling=None,\n",
    "        classes=n_class,\n",
    "#         classifier_activation=last_act_func,\n",
    "    )\n",
    "\n",
    "    model = Sequential(layers=pretrained_model.layers)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(10, activation=last_act_func))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training pipeline\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import applications as tkp\n",
    "import numpy as np\n",
    "\n",
    "def train_model(preprocess_with,model_name, train_images, train_labels, validation_images, validation_labels):\n",
    "\n",
    "    X = preprocess_with.preprocess_input(train_images)\n",
    "    y = train_labels\n",
    "    # y =  tf.keras.utils.to_categorical(train_labels, num_classes = 10)\n",
    "\n",
    "    X_val = preprocess_with.preprocess_input(validation_images)\n",
    "    y_val = validation_labels\n",
    "    # y_val =  tf.keras.utils.to_categorical(validation_labels, num_classes = 10)\n",
    "    print(X.shape, y.shape,X_val.shape, y_val.shape)\n",
    "\n",
    "    model = get_model(model_name, input_shape_=(32,32,3), n_class=10, last_act_func=\"softmax\")\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=SGD(lr=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "    # for layer in model.layers:\n",
    "    #     layer.trainable = True\n",
    "\n",
    "    model_history = model.fit(X,y,\n",
    "          epochs=50,          \n",
    "          validation_data=(X_val,y_val),\n",
    "          validation_freq = 1,\n",
    "          workers = 48,\n",
    "          callbacks=[tensorboard_cb],\n",
    "          use_multiprocessing=True\n",
    "         )\n",
    "    return model, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "821/821 [==============================] - 25s 26ms/step - loss: 1.2068 - accuracy: 0.5826 - val_loss: 1.0606 - val_accuracy: 0.6371\n",
      "Epoch 2/50\n",
      "821/821 [==============================] - 20s 25ms/step - loss: 0.7334 - accuracy: 0.7475 - val_loss: 0.7161 - val_accuracy: 0.7499\n",
      "Epoch 3/50\n",
      "821/821 [==============================] - 20s 24ms/step - loss: 0.5793 - accuracy: 0.7995 - val_loss: 0.8617 - val_accuracy: 0.7229\n",
      "Epoch 4/50\n",
      "821/821 [==============================] - 20s 24ms/step - loss: 0.4696 - accuracy: 0.8371 - val_loss: 0.6412 - val_accuracy: 0.7873\n",
      "Epoch 5/50\n",
      "821/821 [==============================] - 20s 25ms/step - loss: 0.3816 - accuracy: 0.8665 - val_loss: 0.6485 - val_accuracy: 0.7857\n",
      "Epoch 6/50\n",
      "821/821 [==============================] - 20s 25ms/step - loss: 0.3096 - accuracy: 0.8908 - val_loss: 0.6774 - val_accuracy: 0.7873\n",
      "Epoch 7/50\n",
      "821/821 [==============================] - 20s 25ms/step - loss: 0.2444 - accuracy: 0.9158 - val_loss: 1.0130 - val_accuracy: 0.7282\n",
      "Epoch 8/50\n",
      "821/821 [==============================] - 20s 25ms/step - loss: 0.1889 - accuracy: 0.9351 - val_loss: 0.8693 - val_accuracy: 0.7712\n",
      "Epoch 9/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 0.1426 - accuracy: 0.9520 - val_loss: 0.8630 - val_accuracy: 0.7816\n",
      "Epoch 10/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 0.1029 - accuracy: 0.9666 - val_loss: 1.0615 - val_accuracy: 0.7629\n",
      "Epoch 11/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 0.0799 - accuracy: 0.9768 - val_loss: 0.8130 - val_accuracy: 0.8054\n",
      "Epoch 12/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 0.0564 - accuracy: 0.9829 - val_loss: 1.0573 - val_accuracy: 0.7850\n",
      "Epoch 13/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 0.0357 - accuracy: 0.9907 - val_loss: 1.0647 - val_accuracy: 0.7983\n",
      "Epoch 14/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 0.0298 - accuracy: 0.9928 - val_loss: 0.9529 - val_accuracy: 0.7887\n",
      "Epoch 15/50\n",
      "821/821 [==============================] - 21s 26ms/step - loss: 0.0266 - accuracy: 0.9930 - val_loss: 1.1057 - val_accuracy: 0.7969\n",
      "Epoch 16/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 0.0171 - accuracy: 0.9966 - val_loss: 1.1013 - val_accuracy: 0.8154\n",
      "Epoch 17/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 0.0034 - accuracy: 0.9997 - val_loss: 1.2180 - val_accuracy: 0.8185\n",
      "Epoch 18/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.2610 - val_accuracy: 0.8176\n",
      "Epoch 19/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 8.2931e-04 - accuracy: 1.0000 - val_loss: 1.3109 - val_accuracy: 0.8185\n",
      "Epoch 20/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 6.2711e-04 - accuracy: 1.0000 - val_loss: 1.3564 - val_accuracy: 0.8183\n",
      "Epoch 21/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 5.0733e-04 - accuracy: 1.0000 - val_loss: 1.3837 - val_accuracy: 0.8194\n",
      "Epoch 22/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 4.2829e-04 - accuracy: 1.0000 - val_loss: 1.4066 - val_accuracy: 0.8175\n",
      "Epoch 23/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 3.7106e-04 - accuracy: 1.0000 - val_loss: 1.4379 - val_accuracy: 0.8192\n",
      "Epoch 24/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 3.2696e-04 - accuracy: 1.0000 - val_loss: 1.4551 - val_accuracy: 0.8190\n",
      "Epoch 25/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 2.9359e-04 - accuracy: 1.0000 - val_loss: 1.4741 - val_accuracy: 0.8201\n",
      "Epoch 26/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 2.6529e-04 - accuracy: 1.0000 - val_loss: 1.4903 - val_accuracy: 0.8193\n",
      "Epoch 27/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 2.4207e-04 - accuracy: 1.0000 - val_loss: 1.5054 - val_accuracy: 0.8195\n",
      "Epoch 28/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 2.2270e-04 - accuracy: 1.0000 - val_loss: 1.5202 - val_accuracy: 0.8203\n",
      "Epoch 29/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 2.0631e-04 - accuracy: 1.0000 - val_loss: 1.5327 - val_accuracy: 0.8199\n",
      "Epoch 30/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 1.9195e-04 - accuracy: 1.0000 - val_loss: 1.5450 - val_accuracy: 0.8197\n",
      "Epoch 31/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 1.7884e-04 - accuracy: 1.0000 - val_loss: 1.5539 - val_accuracy: 0.8183\n",
      "Epoch 32/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 1.6811e-04 - accuracy: 1.0000 - val_loss: 1.5666 - val_accuracy: 0.8186\n",
      "Epoch 33/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 1.5807e-04 - accuracy: 1.0000 - val_loss: 1.5777 - val_accuracy: 0.8194\n",
      "Epoch 34/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 1.4954e-04 - accuracy: 1.0000 - val_loss: 1.5858 - val_accuracy: 0.8195\n",
      "Epoch 35/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 1.4168e-04 - accuracy: 1.0000 - val_loss: 1.5958 - val_accuracy: 0.8194\n",
      "Epoch 36/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 1.3446e-04 - accuracy: 1.0000 - val_loss: 1.6051 - val_accuracy: 0.8197\n",
      "Epoch 37/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 1.2814e-04 - accuracy: 1.0000 - val_loss: 1.6135 - val_accuracy: 0.8195\n",
      "Epoch 38/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 1.2203e-04 - accuracy: 1.0000 - val_loss: 1.6222 - val_accuracy: 0.8198\n",
      "Epoch 39/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 1.1670e-04 - accuracy: 1.0000 - val_loss: 1.6308 - val_accuracy: 0.8195\n",
      "Epoch 40/50\n",
      "821/821 [==============================] - 21s 26ms/step - loss: 1.1159e-04 - accuracy: 1.0000 - val_loss: 1.6371 - val_accuracy: 0.8195\n",
      "Epoch 41/50\n",
      "821/821 [==============================] - 21s 26ms/step - loss: 1.0709e-04 - accuracy: 1.0000 - val_loss: 1.6454 - val_accuracy: 0.8193\n",
      "Epoch 42/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 1.0292e-04 - accuracy: 1.0000 - val_loss: 1.6518 - val_accuracy: 0.8189\n",
      "Epoch 43/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 9.8857e-05 - accuracy: 1.0000 - val_loss: 1.6578 - val_accuracy: 0.8191\n",
      "Epoch 44/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 9.5157e-05 - accuracy: 1.0000 - val_loss: 1.6657 - val_accuracy: 0.8195\n",
      "Epoch 45/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 9.1854e-05 - accuracy: 1.0000 - val_loss: 1.6717 - val_accuracy: 0.8190\n",
      "Epoch 46/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 8.8697e-05 - accuracy: 1.0000 - val_loss: 1.6782 - val_accuracy: 0.8193\n",
      "Epoch 47/50\n",
      "821/821 [==============================] - 21s 25ms/step - loss: 8.5712e-05 - accuracy: 1.0000 - val_loss: 1.6848 - val_accuracy: 0.8191\n",
      "Epoch 48/50\n",
      "715/821 [=========================>....] - ETA: 2s - loss: 8.2266e-05 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b4c176441c5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#     layer.trainable = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m model.fit(X,y,\n\u001b[0m\u001b[1;32m     18\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1219\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \"\"\"\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \"\"\"\n\u001b[1;32m   1148\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # vgg16_model\n",
    "# model = get_model(VGG19, input_shape_=(32,32,3), n_class=10, last_act_func=\"softmax\")\n",
    "# from tensorflow.keras.optimizers import SGD\n",
    "# model.compile(loss='sparse_categorical_crossentropy',\n",
    "#               optimizer=SGD(lr=0.001),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "# # for layer in model.layers:\n",
    "# #     layer.trainable = True\n",
    "\n",
    "# model.fit(X,y,\n",
    "#           epochs=50,          \n",
    "#           validation_data=(X_val,y_val),\n",
    "#           validation_freq = 1,\n",
    "#           workers = 48,\n",
    "#           callbacks=[tensorboard_cb],\n",
    "#           use_multiprocessing=True\n",
    "#          )\n",
    "\n",
    "# train_model(preprocess_with=tkp.vgg16,model_name=VGG16, train_images, train_labels, validation_images, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 11ms/step - loss: 5.8078 - accuracy: 0.6037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.8077521324157715, 0.6036999821662903]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 1.7145 - accuracy: 0.8191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7145371437072754, 0.819100022315979]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(preprocess_input(test_images), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26250, 32, 32, 3) (26250, 1) (8750, 32, 32, 3) (8750, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "821/821 [==============================] - 26s 30ms/step - loss: 1.1033 - accuracy: 0.6199 - val_loss: 1.2185 - val_accuracy: 0.5855\n",
      "Epoch 2/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.7070 - accuracy: 0.7600 - val_loss: 0.8553 - val_accuracy: 0.7066\n",
      "Epoch 3/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.5715 - accuracy: 0.8053 - val_loss: 0.6781 - val_accuracy: 0.7744\n",
      "Epoch 4/50\n",
      "821/821 [==============================] - 25s 31ms/step - loss: 0.4753 - accuracy: 0.8358 - val_loss: 0.7662 - val_accuracy: 0.7447\n",
      "Epoch 5/50\n",
      "821/821 [==============================] - 25s 31ms/step - loss: 0.3958 - accuracy: 0.8633 - val_loss: 0.7885 - val_accuracy: 0.7427\n",
      "Epoch 6/50\n",
      "821/821 [==============================] - 25s 31ms/step - loss: 0.3297 - accuracy: 0.8870 - val_loss: 0.6672 - val_accuracy: 0.7918\n",
      "Epoch 7/50\n",
      "821/821 [==============================] - 25s 31ms/step - loss: 0.2699 - accuracy: 0.9073 - val_loss: 0.7263 - val_accuracy: 0.7861\n",
      "Epoch 8/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 0.2148 - accuracy: 0.9261 - val_loss: 0.6692 - val_accuracy: 0.7941\n",
      "Epoch 9/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 0.1689 - accuracy: 0.9420 - val_loss: 0.7019 - val_accuracy: 0.8078\n",
      "Epoch 10/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 0.1325 - accuracy: 0.9543 - val_loss: 0.9878 - val_accuracy: 0.7539\n",
      "Epoch 11/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 0.1135 - accuracy: 0.9631 - val_loss: 1.3033 - val_accuracy: 0.7208\n",
      "Epoch 12/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 0.0816 - accuracy: 0.9746 - val_loss: 0.8698 - val_accuracy: 0.8115\n",
      "Epoch 13/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 0.0635 - accuracy: 0.9795 - val_loss: 0.9069 - val_accuracy: 0.8040\n",
      "Epoch 14/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 0.0446 - accuracy: 0.9868 - val_loss: 1.2954 - val_accuracy: 0.7703\n",
      "Epoch 15/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 0.0340 - accuracy: 0.9906 - val_loss: 1.1152 - val_accuracy: 0.7912\n",
      "Epoch 16/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 0.0437 - accuracy: 0.9878 - val_loss: 0.9873 - val_accuracy: 0.8125\n",
      "Epoch 17/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 0.0239 - accuracy: 0.9944 - val_loss: 1.0079 - val_accuracy: 0.8082\n",
      "Epoch 18/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 0.0221 - accuracy: 0.9955 - val_loss: 1.0241 - val_accuracy: 0.8091\n",
      "Epoch 19/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 0.0193 - accuracy: 0.9963 - val_loss: 0.9017 - val_accuracy: 0.7897\n",
      "Epoch 20/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 0.0231 - accuracy: 0.9951 - val_loss: 1.1230 - val_accuracy: 0.8162\n",
      "Epoch 21/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 1.2359 - val_accuracy: 0.8183\n",
      "Epoch 22/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 7.8996e-04 - accuracy: 1.0000 - val_loss: 1.3354 - val_accuracy: 0.8192\n",
      "Epoch 23/50\n",
      "821/821 [==============================] - 25s 31ms/step - loss: 5.1253e-04 - accuracy: 1.0000 - val_loss: 1.3936 - val_accuracy: 0.8194\n",
      "Epoch 24/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 3.9007e-04 - accuracy: 1.0000 - val_loss: 1.4319 - val_accuracy: 0.8205\n",
      "Epoch 25/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 3.1754e-04 - accuracy: 1.0000 - val_loss: 1.4659 - val_accuracy: 0.8199\n",
      "Epoch 26/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 2.6716e-04 - accuracy: 1.0000 - val_loss: 1.4956 - val_accuracy: 0.8210\n",
      "Epoch 27/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 2.3239e-04 - accuracy: 1.0000 - val_loss: 1.5171 - val_accuracy: 0.8200\n",
      "Epoch 28/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 2.0453e-04 - accuracy: 1.0000 - val_loss: 1.5429 - val_accuracy: 0.8206\n",
      "Epoch 29/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 1.8357e-04 - accuracy: 1.0000 - val_loss: 1.5598 - val_accuracy: 0.8210\n",
      "Epoch 30/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 1.6580e-04 - accuracy: 1.0000 - val_loss: 1.5749 - val_accuracy: 0.8205\n",
      "Epoch 31/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 1.5127e-04 - accuracy: 1.0000 - val_loss: 1.5911 - val_accuracy: 0.8207\n",
      "Epoch 32/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 1.3900e-04 - accuracy: 1.0000 - val_loss: 1.6095 - val_accuracy: 0.8203\n",
      "Epoch 33/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 1.2882e-04 - accuracy: 1.0000 - val_loss: 1.6207 - val_accuracy: 0.8209\n",
      "Epoch 34/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 1.1962e-04 - accuracy: 1.0000 - val_loss: 1.6340 - val_accuracy: 0.8206\n",
      "Epoch 35/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 1.1202e-04 - accuracy: 1.0000 - val_loss: 1.6489 - val_accuracy: 0.8207\n",
      "Epoch 36/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 1.0482e-04 - accuracy: 1.0000 - val_loss: 1.6604 - val_accuracy: 0.8208\n",
      "Epoch 37/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 9.8944e-05 - accuracy: 1.0000 - val_loss: 1.6697 - val_accuracy: 0.8210\n",
      "Epoch 38/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 9.3364e-05 - accuracy: 1.0000 - val_loss: 1.6811 - val_accuracy: 0.8206\n",
      "Epoch 39/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 8.8369e-05 - accuracy: 1.0000 - val_loss: 1.6924 - val_accuracy: 0.8203\n",
      "Epoch 40/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 8.3932e-05 - accuracy: 1.0000 - val_loss: 1.7021 - val_accuracy: 0.8207\n",
      "Epoch 41/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 7.9917e-05 - accuracy: 1.0000 - val_loss: 1.7120 - val_accuracy: 0.8203\n",
      "Epoch 42/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 7.6274e-05 - accuracy: 1.0000 - val_loss: 1.7186 - val_accuracy: 0.8207\n",
      "Epoch 43/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 7.2881e-05 - accuracy: 1.0000 - val_loss: 1.7263 - val_accuracy: 0.8211\n",
      "Epoch 44/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 6.9858e-05 - accuracy: 1.0000 - val_loss: 1.7343 - val_accuracy: 0.8213\n",
      "Epoch 45/50\n",
      "821/821 [==============================] - 26s 32ms/step - loss: 6.7023e-05 - accuracy: 1.0000 - val_loss: 1.7439 - val_accuracy: 0.8208\n",
      "Epoch 46/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 6.4380e-05 - accuracy: 1.0000 - val_loss: 1.7528 - val_accuracy: 0.8207\n",
      "Epoch 47/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 6.1971e-05 - accuracy: 1.0000 - val_loss: 1.7595 - val_accuracy: 0.8211\n",
      "Epoch 48/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 5.9654e-05 - accuracy: 1.0000 - val_loss: 1.7661 - val_accuracy: 0.8208\n",
      "Epoch 49/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 5.7573e-05 - accuracy: 1.0000 - val_loss: 1.7741 - val_accuracy: 0.8209\n",
      "Epoch 50/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 5.5538e-05 - accuracy: 1.0000 - val_loss: 1.7807 - val_accuracy: 0.8209\n"
     ]
    }
   ],
   "source": [
    "model, history = train_model(preprocess_with=tkp.vgg19,model_name=VGG19, train_images=train_images, train_labels=train_labels, validation_images=validation_images, validation_labels=validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 11ms/step - loss: 1.7843 - accuracy: 0.8229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7842907905578613, 0.8228999972343445]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(preprocess_input(test_images), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('Saved_model/CIFAR10_VGG19_epochs.h5')\n",
    "\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26250, 32, 32, 3) (26250, 1) (8750, 32, 32, 3) (8750, 1)\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 2s 0us/step\n",
      "17235968/17225924 [==============================] - 2s 0us/step\n",
      "Epoch 1/50\n",
      "821/821 [==============================] - 29s 32ms/step - loss: 2.0302 - accuracy: 0.2892 - val_loss: 1.6374 - val_accuracy: 0.4360\n",
      "Epoch 2/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 1.4459 - accuracy: 0.4952 - val_loss: 1.3299 - val_accuracy: 0.5367\n",
      "Epoch 3/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 1.2513 - accuracy: 0.5645 - val_loss: 1.1922 - val_accuracy: 0.5807\n",
      "Epoch 4/50\n",
      "821/821 [==============================] - 24s 30ms/step - loss: 1.1401 - accuracy: 0.6044 - val_loss: 1.0934 - val_accuracy: 0.6161\n",
      "Epoch 5/50\n",
      "821/821 [==============================] - 25s 31ms/step - loss: 1.0651 - accuracy: 0.6334 - val_loss: 1.0472 - val_accuracy: 0.6317\n",
      "Epoch 6/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.9947 - accuracy: 0.6560 - val_loss: 0.9940 - val_accuracy: 0.6513\n",
      "Epoch 7/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.9477 - accuracy: 0.6739 - val_loss: 0.9599 - val_accuracy: 0.6649\n",
      "Epoch 8/50\n",
      "821/821 [==============================] - 24s 29ms/step - loss: 0.9062 - accuracy: 0.6860 - val_loss: 0.9325 - val_accuracy: 0.6750\n",
      "Epoch 9/50\n",
      "821/821 [==============================] - 25s 31ms/step - loss: 0.8657 - accuracy: 0.7013 - val_loss: 0.9039 - val_accuracy: 0.6909\n",
      "Epoch 10/50\n",
      "821/821 [==============================] - 25s 31ms/step - loss: 0.8167 - accuracy: 0.7175 - val_loss: 0.8776 - val_accuracy: 0.6945\n",
      "Epoch 11/50\n",
      "821/821 [==============================] - 26s 31ms/step - loss: 0.7924 - accuracy: 0.7266 - val_loss: 0.8515 - val_accuracy: 0.7049\n",
      "Epoch 12/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.7703 - accuracy: 0.7318 - val_loss: 0.8498 - val_accuracy: 0.7024\n",
      "Epoch 13/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.7461 - accuracy: 0.7431 - val_loss: 0.8286 - val_accuracy: 0.7121\n",
      "Epoch 14/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.7237 - accuracy: 0.7461 - val_loss: 0.8153 - val_accuracy: 0.7112\n",
      "Epoch 15/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.6986 - accuracy: 0.7570 - val_loss: 0.7996 - val_accuracy: 0.7214\n",
      "Epoch 16/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.6690 - accuracy: 0.7690 - val_loss: 0.7909 - val_accuracy: 0.7243\n",
      "Epoch 17/50\n",
      "821/821 [==============================] - 24s 29ms/step - loss: 0.6526 - accuracy: 0.7729 - val_loss: 0.7833 - val_accuracy: 0.7250\n",
      "Epoch 18/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.6250 - accuracy: 0.7831 - val_loss: 0.7686 - val_accuracy: 0.7297\n",
      "Epoch 19/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.6108 - accuracy: 0.7882 - val_loss: 0.7632 - val_accuracy: 0.7365\n",
      "Epoch 20/50\n",
      "821/821 [==============================] - 24s 30ms/step - loss: 0.5922 - accuracy: 0.7914 - val_loss: 0.7684 - val_accuracy: 0.7358\n",
      "Epoch 21/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.5773 - accuracy: 0.7963 - val_loss: 0.7596 - val_accuracy: 0.7402\n",
      "Epoch 22/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.5539 - accuracy: 0.8068 - val_loss: 0.7500 - val_accuracy: 0.7408\n",
      "Epoch 23/50\n",
      "821/821 [==============================] - 24s 29ms/step - loss: 0.5372 - accuracy: 0.8131 - val_loss: 0.7498 - val_accuracy: 0.7415\n",
      "Epoch 24/50\n",
      "821/821 [==============================] - 24s 29ms/step - loss: 0.5232 - accuracy: 0.8158 - val_loss: 0.7536 - val_accuracy: 0.7378\n",
      "Epoch 25/50\n",
      "821/821 [==============================] - 25s 31ms/step - loss: 0.5078 - accuracy: 0.8232 - val_loss: 0.7443 - val_accuracy: 0.7479\n",
      "Epoch 26/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.4936 - accuracy: 0.8264 - val_loss: 0.7518 - val_accuracy: 0.7441\n",
      "Epoch 27/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.4825 - accuracy: 0.8319 - val_loss: 0.7435 - val_accuracy: 0.7510\n",
      "Epoch 28/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.4749 - accuracy: 0.8338 - val_loss: 0.7342 - val_accuracy: 0.7499\n",
      "Epoch 29/50\n",
      "821/821 [==============================] - 24s 29ms/step - loss: 0.4485 - accuracy: 0.8421 - val_loss: 0.7303 - val_accuracy: 0.7510\n",
      "Epoch 30/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.4428 - accuracy: 0.8443 - val_loss: 0.7334 - val_accuracy: 0.7527\n",
      "Epoch 31/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.4304 - accuracy: 0.8489 - val_loss: 0.7413 - val_accuracy: 0.7472\n",
      "Epoch 32/50\n",
      "821/821 [==============================] - 24s 29ms/step - loss: 0.4171 - accuracy: 0.8527 - val_loss: 0.7334 - val_accuracy: 0.7490\n",
      "Epoch 33/50\n",
      "821/821 [==============================] - 24s 29ms/step - loss: 0.4001 - accuracy: 0.8605 - val_loss: 0.7407 - val_accuracy: 0.7501\n",
      "Epoch 34/50\n",
      "821/821 [==============================] - 24s 29ms/step - loss: 0.3932 - accuracy: 0.8657 - val_loss: 0.7342 - val_accuracy: 0.7505\n",
      "Epoch 35/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.3849 - accuracy: 0.8634 - val_loss: 0.7468 - val_accuracy: 0.7518\n",
      "Epoch 36/50\n",
      "821/821 [==============================] - 24s 30ms/step - loss: 0.3742 - accuracy: 0.8678 - val_loss: 0.7403 - val_accuracy: 0.7566\n",
      "Epoch 37/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.3628 - accuracy: 0.8733 - val_loss: 0.7519 - val_accuracy: 0.7520\n",
      "Epoch 38/50\n",
      "821/821 [==============================] - 24s 30ms/step - loss: 0.3461 - accuracy: 0.8764 - val_loss: 0.7493 - val_accuracy: 0.7534\n",
      "Epoch 39/50\n",
      "821/821 [==============================] - 24s 30ms/step - loss: 0.3448 - accuracy: 0.8795 - val_loss: 0.7487 - val_accuracy: 0.7546\n",
      "Epoch 40/50\n",
      "821/821 [==============================] - 24s 30ms/step - loss: 0.3313 - accuracy: 0.8833 - val_loss: 0.7514 - val_accuracy: 0.7591\n",
      "Epoch 41/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.3211 - accuracy: 0.8883 - val_loss: 0.7694 - val_accuracy: 0.7513\n",
      "Epoch 42/50\n",
      "821/821 [==============================] - 24s 30ms/step - loss: 0.3127 - accuracy: 0.8944 - val_loss: 0.7615 - val_accuracy: 0.7579\n",
      "Epoch 43/50\n",
      "821/821 [==============================] - 24s 29ms/step - loss: 0.3042 - accuracy: 0.8919 - val_loss: 0.7627 - val_accuracy: 0.7592\n",
      "Epoch 44/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.2869 - accuracy: 0.9005 - val_loss: 0.7712 - val_accuracy: 0.7579\n",
      "Epoch 45/50\n",
      "821/821 [==============================] - 24s 29ms/step - loss: 0.2853 - accuracy: 0.9005 - val_loss: 0.7679 - val_accuracy: 0.7578\n",
      "Epoch 46/50\n",
      "821/821 [==============================] - 24s 30ms/step - loss: 0.2715 - accuracy: 0.9062 - val_loss: 0.7716 - val_accuracy: 0.7614\n",
      "Epoch 47/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.2639 - accuracy: 0.9077 - val_loss: 0.7793 - val_accuracy: 0.7614\n",
      "Epoch 48/50\n",
      "821/821 [==============================] - 24s 30ms/step - loss: 0.2619 - accuracy: 0.9096 - val_loss: 0.7846 - val_accuracy: 0.7606\n",
      "Epoch 49/50\n",
      "821/821 [==============================] - 25s 31ms/step - loss: 0.2478 - accuracy: 0.9109 - val_loss: 0.7786 - val_accuracy: 0.7621\n",
      "Epoch 50/50\n",
      "821/821 [==============================] - 25s 30ms/step - loss: 0.2534 - accuracy: 0.9110 - val_loss: 0.7933 - val_accuracy: 0.7576\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import Xception , MobileNet\n",
    "# Xception - minimum input size 71 X 71  is req.\n",
    "# MobileNetV2 - minimum input size 71 X 71  is req.\n",
    "\n",
    "model, history = train_model(preprocess_with=tkp.mobilenet,\n",
    "                             model_name=MobileNet,\n",
    "                             train_images=train_images, train_labels=train_labels, \n",
    "                             validation_images=validation_images, validation_labels=validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 14ms/step - loss: 3.7610 - accuracy: 0.2138\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(preprocess_input(test_images), test_labels)\n",
    "model.save('Saved_model/CIFAR10_MobileNet_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 13ms/step - loss: 3.7610 - accuracy: 0.2138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.76102614402771, 0.21379999816417694]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(preprocess_input(test_images), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
